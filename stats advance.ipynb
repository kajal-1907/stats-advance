{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7929295a-cfe1-4948-9f0f-19fc573e6f1b",
   "metadata": {},
   "source": [
    "1. Explain the properties of the F-distribution. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8c589d-41e2-489a-a750-3d31f410c700",
   "metadata": {},
   "source": [
    "The F-distribution is a continuous probability distribution that arises frequently in the context of statistical inference, particularly in analysis of variance (ANOVA) and regression analysis. Here are some key properties of the F-distribution:\n",
    "\n",
    "1. **Definition**: The F-distribution is defined as the ratio of two scaled chi-squared distributions. Specifically, if \\( X \\) and \\( Y \\) are independent chi-squared variables with \\( d_1 \\) and \\( d_2 \\) degrees of freedom respectively, then the variable \\( F = \\frac{(X/d_1)}{(Y/d_2)} \\) follows an F-distribution with \\( d_1 \\) and \\( d_2 \\) degrees of freedom.\n",
    "\n",
    "2. **Shape**: The F-distribution is positively skewed, meaning it has a longer tail on the right. As the degrees of freedom increase, the distribution becomes more symmetric and approaches a normal distribution.\n",
    "\n",
    "3. **Degrees of Freedom**: The shape of the F-distribution depends on two parameters: \\( d_1 \\) (the degrees of freedom for the numerator) and \\( d_2 \\) (the degrees of freedom for the denominator). Different combinations of these parameters yield different shapes of the distribution.\n",
    "\n",
    "4. **Mean**: The mean of the F-distribution is given by:\n",
    "   \\[\n",
    "   \\text{Mean} = \\frac{d_2}{d_2 - 2} \\quad \\text{for } d_2 > 2\n",
    "   \\]\n",
    "   If \\( d_2 \\leq 2 \\), the mean is undefined.\n",
    "\n",
    "5. **Variance**: The variance of the F-distribution is:\n",
    "   \\[\n",
    "   \\text{Variance} = \\frac{2d_2^2(d_1 + d_1 - 2)}{d_1(d_2 - 2)^2(d_2 - 4)} \\quad \\text{for } d_2 > 4\n",
    "   \\]\n",
    "   Like the mean, the variance is undefined for \\( d_2 \\leq 4 \\).\n",
    "\n",
    "6. **Range**: The values of the F-distribution range from 0 to \\( \\infty \\).\n",
    "\n",
    "7. **Cumulative Distribution Function (CDF)**: The CDF of the F-distribution can be computed using integral calculus or lookup tables, and it is used to determine probabilities associated with the F-statistic.\n",
    "\n",
    "8. **Applications**: The F-distribution is primarily used in hypothesis testing, especially in comparing variances and in ANOVA tests. It helps to determine whether the means of different groups are significantly different from each other.\n",
    "\n",
    "9. **Non-Negative**: The F-distribution only takes on non-negative values, as it is based on squared terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fab5ff-5e34-4a26-9e26-1625f1dce779",
   "metadata": {},
   "source": [
    "2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36703e12-b4a1-4e1b-b5ed-a531d2410a16",
   "metadata": {},
   "source": [
    "The F-distribution is primarily used in the following types of statistical tests:\n",
    "\n",
    "1. **Analysis of Variance (ANOVA)**: ANOVA tests are used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others. The F-distribution is appropriate here because the test statistic is calculated as the ratio of between-group variance to within-group variance, which follows an F-distribution under the null hypothesis.\n",
    "\n",
    "2. **Regression Analysis**: In multiple regression, the F-test is used to assess the overall significance of the regression model. It compares the model with predictors to a model with no predictors (the null model). The F-statistic here is based on the ratio of the explained variance to the unexplained variance, which follows an F-distribution.\n",
    "\n",
    "3. **Comparing Two Variances**: The F-test can be used to compare the variances of two populations. If you have two independent samples, the ratio of their sample variances follows an F-distribution under the null hypothesis that the variances are equal. This is useful in determining if the variability in two groups is statistically different.\n",
    "\n",
    "4. **Design of Experiments**: In factorial experiments, the F-distribution is used to analyze the interaction effects among multiple factors. Each interaction effect can be tested using an F-statistic to determine if the interaction significantly affects the response variable.\n",
    "\n",
    "### Why the F-distribution is Appropriate:\n",
    "\n",
    "1. **Ratio of Variances**: The F-distribution is specifically designed for situations where the test statistic involves a ratio of variances, making it a natural choice for tests like ANOVA and variance comparisons.\n",
    "\n",
    "2. **Asymptotic Behavior**: As sample sizes increase, the F-distribution approaches normality, allowing for reliable inference as sample sizes grow.\n",
    "\n",
    "3. **Non-Negative Values**: The F-distribution only takes non-negative values, which is appropriate since variances cannot be negative.\n",
    "\n",
    "4. **Flexibility with Degrees of Freedom**: The F-distribution can be shaped by adjusting its degrees of freedom, allowing it to model a wide range of scenarios encountered in statistical testing.\n",
    "\n",
    "5. **Underlying Assumptions**: Many tests using the F-distribution assume that the populations being compared are normally distributed, which aligns with the assumptions of ANOVA and regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed0834-9abc-4e4d-9615-d14bce4308d7",
   "metadata": {},
   "source": [
    "3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
    "populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7ac71e-33ec-4401-aaf5-e8a74f56f0cd",
   "metadata": {},
   "source": [
    "To conduct an F-test for comparing the variances of two populations, several key assumptions must be met:\n",
    "\n",
    "1. **Independence**: The two samples being compared must be independent of each other. This means that the selection or outcome of one sample does not influence the other.\n",
    "\n",
    "2. **Normality**: The populations from which the samples are drawn should be normally distributed. While the F-test is somewhat robust to violations of this assumption, significant departures from normality can affect the validity of the test results, especially with small sample sizes.\n",
    "\n",
    "3. **Random Sampling**: The samples should be randomly selected from the populations. This ensures that the samples are representative and reduces bias.\n",
    "\n",
    "4. **Equal Variances**: Although the F-test is designed to compare variances, it assumes that the data is appropriately distributed under the null hypothesis that the variances are equal. This is crucial for accurately interpreting the F-statistic.\n",
    "\n",
    "5. **Scale of Measurement**: The data should be measured on an interval or ratio scale, allowing for meaningful comparisons of variances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958db8c4-f9ae-473d-ab86-b1e790e824cd",
   "metadata": {},
   "source": [
    "4. What is the purpose of ANOVA, and how does it differ from a t-test? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ddeea-64f0-4994-bb19-7e4b66a8e87f",
   "metadata": {},
   "source": [
    "ANOVA (Analysis of Variance) is a statistical method used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others. Here are the key purposes and how it differs from a t-test:\n",
    "\n",
    "### Purpose of ANOVA:\n",
    "\n",
    "1. **Comparison of Multiple Groups**: ANOVA is designed to assess differences among three or more groups simultaneously, which is more efficient than conducting multiple t-tests.\n",
    "\n",
    "2. **Control of Type I Error**: By using ANOVA, researchers can minimize the risk of committing a Type I error (incorrectly rejecting the null hypothesis) that can arise when performing multiple pairwise t-tests.\n",
    "\n",
    "3. **Assessment of Variability**: ANOVA evaluates the variability within groups compared to the variability between groups, helping to determine whether the observed group differences are statistically significant.\n",
    "\n",
    "### Differences from a t-test:\n",
    "\n",
    "1. **Number of Groups**: \n",
    "   - **ANOVA**: Used for comparing means across three or more groups.\n",
    "   - **t-test**: Used for comparing the means of two groups.\n",
    "\n",
    "2. **Hypotheses**:\n",
    "   - **ANOVA**: Tests the null hypothesis that all group means are equal (e.g., \\( H_0: \\mu_1 = \\mu_2 = \\mu_3 \\)).\n",
    "   - **t-test**: Tests the null hypothesis that the means of the two groups are equal (e.g., \\( H_0: \\mu_1 = \\mu_2 \\)).\n",
    "\n",
    "3. **Type of Analysis**:\n",
    "   - **ANOVA**: Provides a single F-statistic that indicates whether there is a significant difference among group means. If significant, post-hoc tests (like Tukey's or Bonferroni) are often conducted to determine which specific groups differ.\n",
    "   - **t-test**: Produces a t-statistic and directly compares the means of the two groups, providing a p-value to assess significance.\n",
    "\n",
    "4. **Complexity of Designs**:\n",
    "   - **ANOVA**: Can handle more complex designs, including factorial designs that examine interactions between multiple factors.\n",
    "   - **t-test**: Generally limited to simpler comparisons between two groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c6f980-3ab8-47c7-bb83-cb14223294e2",
   "metadata": {},
   "source": [
    "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
    "than two groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5570863-a8b5-485b-b326-d75af9c30340",
   "metadata": {},
   "source": [
    "Using a one-way ANOVA instead of multiple t-tests is important for several reasons when comparing more than two groups:\n",
    "\n",
    "### When to Use One-Way ANOVA:\n",
    "\n",
    "1. **Comparing Three or More Groups**: One-way ANOVA is specifically designed for situations where you want to compare the means of three or more groups simultaneously. If you have more than two groups to analyze, ANOVA is the appropriate choice.\n",
    "\n",
    "### Why Use One-Way ANOVA Instead of Multiple t-Tests:\n",
    "\n",
    "1. **Control of Type I Error**: Each t-test carries a risk of committing a Type I error (incorrectly rejecting the null hypothesis). When you conduct multiple t-tests, the overall probability of making at least one Type I error increases. One-way ANOVA addresses this issue by providing a single test to compare all groups, thus controlling the overall error rate.\n",
    "\n",
    "2. **Efficiency**: Conducting multiple t-tests can be time-consuming and may lead to confusion regarding which differences are significant. One-way ANOVA allows for a single analysis to evaluate the differences among all groups, making the process more efficient and streamlined.\n",
    "\n",
    "3. **Comprehensive Comparison**: ANOVA evaluates the variance between groups compared to the variance within groups, giving a clearer picture of whether any group differs significantly from the others. This holistic approach is more informative than a series of pairwise comparisons.\n",
    "\n",
    "4. **Post-Hoc Testing**: If the ANOVA indicates significant differences, you can follow up with post-hoc tests (like Tukey’s or Bonferroni) to identify specifically which groups differ. This stepwise approach is not as straightforward with multiple t-tests.\n",
    "\n",
    "5. **Assumption Checking**: One-way ANOVA allows for a more structured approach to checking assumptions (such as normality and homogeneity of variances) collectively for all groups, rather than for each pair of groups as would be necessary with t-tests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18656945-e341-460b-a87e-62577158ee88",
   "metadata": {},
   "source": [
    "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
    "How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a24952-ac46-4e36-acf3-c4e1bf27f8b7",
   "metadata": {},
   "source": [
    "In ANOVA, variance is partitioned into two main components: between-group variance and within-group variance. This partitioning is fundamental to understanding how ANOVA works and contributes to the calculation of the F-statistic.\n",
    "\n",
    "### Partitioning of Variance:\n",
    "\n",
    "1. **Total Variance**: The total variance in the data is the overall variability of all observations from the grand mean (the mean of all data points). It is calculated as:\n",
    "   \\[\n",
    "   \\text{Total Variance} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_{\\text{grand}})^2\n",
    "   \\]\n",
    "   where \\( Y_{ij} \\) represents individual observations, \\( k \\) is the number of groups, \\( n_i \\) is the number of observations in group \\( i \\), and \\( \\bar{Y}_{\\text{grand}} \\) is the grand mean.\n",
    "\n",
    "2. **Between-Group Variance**: This measures how much the group means vary from the grand mean. It reflects the variability attributed to the differences between the groups. It is calculated as:\n",
    "   \\[\n",
    "   \\text{Between-Group Variance} = \\sum_{i=1}^{k} n_i (\\bar{Y}_i - \\bar{Y}_{\\text{grand}})^2\n",
    "   \\]\n",
    "   where \\( \\bar{Y}_i \\) is the mean of group \\( i \\).\n",
    "\n",
    "3. **Within-Group Variance**: This measures the variability of observations within each group. It reflects how much individual observations in each group deviate from their respective group means. It is calculated as:\n",
    "   \\[\n",
    "   \\text{Within-Group Variance} = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} (Y_{ij} - \\bar{Y}_i)^2\n",
    "   \\]\n",
    "\n",
    "### Contribution to the F-Statistic:\n",
    "\n",
    "The F-statistic is a ratio that compares the between-group variance to the within-group variance, helping to assess whether the group means are significantly different from each other.\n",
    "\n",
    "1. **Calculating the F-Statistic**:\n",
    "   The F-statistic is calculated as:\n",
    "   \\[\n",
    "   F = \\frac{\\text{Mean Square Between}}{\\text{Mean Square Within}} = \\frac{MSB}{MSW}\n",
    "   \\]\n",
    "   where:\n",
    "   - \\( MSB \\) (Mean Square Between) is obtained by dividing the between-group variance by its degrees of freedom (df), typically \\( k - 1 \\), where \\( k \\) is the number of groups.\n",
    "   - \\( MSW \\) (Mean Square Within) is obtained by dividing the within-group variance by its degrees of freedom (df), typically \\( N - k \\), where \\( N \\) is the total number of observations.\n",
    "\n",
    "2. **Interpretation**:\n",
    "   - A large F-statistic indicates that a substantial amount of variance is explained by the group differences relative to the variance within groups. This suggests that at least one group mean is significantly different from the others.\n",
    "   - A small F-statistic indicates that the variability among group means is similar to the variability within groups, suggesting no significant differences between the groups."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1f35ab-2a70-4948-af69-10b511bf2584",
   "metadata": {},
   "source": [
    "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
    "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c19f5eb-9e9e-4fd0-92a8-ff9c956758af",
   "metadata": {},
   "source": [
    "Comparing the classical (frequentist) approach to ANOVA with the Bayesian approach reveals several key differences in how they handle uncertainty, parameter estimation, and hypothesis testing. Here’s a breakdown of these differences:\n",
    "\n",
    "### 1. Handling Uncertainty\n",
    "\n",
    "**Frequentist Approach:**\n",
    "- Uncertainty is quantified through confidence intervals and p-values.\n",
    "- It focuses on the probability of observing data under the null hypothesis, treating parameters as fixed but unknown values.\n",
    "- Results are often interpreted in terms of long-run frequencies.\n",
    "\n",
    "**Bayesian Approach:**\n",
    "- Uncertainty is represented using probability distributions for parameters, allowing for a direct interpretation of uncertainty in terms of belief or credence.\n",
    "- It incorporates prior information about parameters, updating this prior with data to obtain a posterior distribution.\n",
    "- The approach treats parameters as random variables, reflecting uncertainty more naturally.\n",
    "\n",
    "### 2. Parameter Estimation\n",
    "\n",
    "**Frequentist Approach:**\n",
    "- Parameter estimation typically involves point estimates (e.g., sample means) and confidence intervals derived from the sampling distribution.\n",
    "- The emphasis is on estimators that have desirable properties, such as unbiasedness and consistency, based on repeated sampling.\n",
    "\n",
    "**Bayesian Approach:**\n",
    "- Parameters are estimated using the posterior distribution, which combines prior beliefs and the likelihood of the observed data.\n",
    "- Point estimates can be derived from the posterior (e.g., the mean, median, or mode), but full uncertainty is captured by the entire posterior distribution.\n",
    "- This approach allows for more flexible modeling and incorporation of prior knowledge.\n",
    "\n",
    "### 3. Hypothesis Testing\n",
    "\n",
    "**Frequentist Approach:**\n",
    "- Hypothesis testing is typically conducted using p-values, with a focus on whether to reject or fail to reject the null hypothesis based on a pre-set significance level (e.g., α = 0.05).\n",
    "- The method relies on a frequentist framework where decisions are made based on the long-run behavior of tests.\n",
    "\n",
    "**Bayesian Approach:**\n",
    "- Bayesian hypothesis testing involves calculating the posterior probabilities of hypotheses, allowing for a more nuanced comparison between them.\n",
    "- Instead of p-values, it can provide Bayes Factors, which quantify the strength of evidence for one hypothesis over another.\n",
    "- This approach allows for the direct evaluation of the probability of a hypothesis given the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1402f86b-8aa9-4422-9d3b-0064da648688",
   "metadata": {},
   "source": [
    "8. Question: You have two sets of data representing the incomes of two different professions1\n",
    "V Profession A: [48, 52, 55, 60, 62'\n",
    "V Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions'\n",
    "incomes are equal. What are your conclusions based on the F-test?\n",
    "\n",
    "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
    "\n",
    "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac37af80-2a73-4cc4-afc8-73faf554a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 2.0892\n",
      "p-value: 0.4930\n",
      "Fail to reject the null hypothesis: no significant difference in variances.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Data for the two professions\n",
    "profession_A = [48, 52, 55, 60, 62]\n",
    "profession_B = [45, 50, 55, 52, 47]\n",
    "\n",
    "# Calculate variances\n",
    "var_A = np.var(profession_A, ddof=1)  # Sample variance\n",
    "var_B = np.var(profession_B, ddof=1)  # Sample variance\n",
    "\n",
    "# Calculate the F-statistic\n",
    "F_statistic = var_A / var_B\n",
    "\n",
    "# Degrees of freedom\n",
    "df_A = len(profession_A) - 1  # df for A\n",
    "df_B = len(profession_B) - 1  # df for B\n",
    "\n",
    "# Calculate the p-value\n",
    "p_value = stats.f.sf(F_statistic, df_A, df_B) * 2  # Two-tailed test\n",
    "\n",
    "# Output the results\n",
    "print(f\"F-statistic: {F_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Conclusion based on a significance level (e.g., alpha = 0.05)\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    conclusion = \"Reject the null hypothesis: variances are significantly different.\"\n",
    "else:\n",
    "    conclusion = \"Fail to reject the null hypothesis: no significant difference in variances.\"\n",
    "\n",
    "print(conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f8871-a9f3-449c-8ac1-fdb526773107",
   "metadata": {},
   "source": [
    "Explanation of the Code\n",
    "\n",
    "1.Data Initialization: The incomes for both professions are stored in lists.\n",
    "\n",
    "2.Variance Calculation: The sample variance is calculated using np.var() with ddof=1 to obtain an unbiased estimator.\n",
    "\n",
    "3.F-statistic Calculation: The F-statistic is computed as the ratio of the two variances.\n",
    "\n",
    "4.Degrees of Freedom: Degrees of freedom for each group are calculated.\n",
    "\n",
    "5.P-value Calculation: The p-value is computed using the cumulative distribution function of the F-distribution, multiplied by 2 for a two-tailed test.\n",
    "\n",
    "6.Conclusion: The result is interpreted based on a significance level (alpha), usually set at 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1bbc94-701a-4480-8aab-fe276cfd3086",
   "metadata": {},
   "source": [
    "9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
    "average heights between three different regions with the following data1\n",
    "V Region A: [160, 162, 165, 158, 164'\n",
    "V Region B: [172, 175, 170, 168, 174'\n",
    "V Region C: [180, 182, 179, 185, 183'\n",
    "V Task: Write Python code to perform the one-way ANOVA and interpret the results\f",
    "\n",
    "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ef7882ae-a50d-4984-8188-3132f418959b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.8733\n",
      "p-value: 0.0000\n",
      "Reject the null hypothesis: there are statistically significant differences in average heights.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Heights data for three regions\n",
    "region_A = [160, 162, 165, 158, 164]\n",
    "region_B = [172, 175, 170, 168, 174]\n",
    "region_C = [180, 182, 179, 185, 183]\n",
    "\n",
    "# Perform one-way ANOVA\n",
    "F_statistic, p_value = stats.f_oneway(region_A, region_B, region_C)\n",
    "\n",
    "# Output the results\n",
    "print(f\"F-statistic: {F_statistic:.4f}\")\n",
    "print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "# Conclusion based on a significance level (e.g., alpha = 0.05)\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    conclusion = \"Reject the null hypothesis: there are statistically significant differences in average heights.\"\n",
    "else:\n",
    "    conclusion = \"Fail to reject the null hypothesis: no significant differences in average heights.\"\n",
    "\n",
    "print(conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23424024-db98-476a-8f93-4c7d3b4dfbb4",
   "metadata": {},
   "source": [
    "Explanation of the Code\n",
    "\n",
    "1.Data Initialization: Heights for the three regions are defined in lists.\n",
    "\n",
    "2.ANOVA Calculation: The f_oneway function calculates the F-statistic and p-value for the input data.\n",
    "\n",
    "3.Output Results: The F-statistic and p-value are printed for interpretation.\n",
    "4.Conclusion: The result is assessed based on a significance level (alpha), commonly set at 0.05.\n",
    "\n",
    "Interpretation of Results\n",
    "1.F-statistic: A higher F-statistic indicates a greater variance among the group means compared to the variance within the groups.\n",
    "p-value:\n",
    "\n",
    "If the p-value is less than 0.05, you reject the null hypothesis, suggesting that at least one region has a significantly different average height.\n",
    "\n",
    "If the p-value is greater than or equal to 0.05, you fail to reject the null hypothesis, indicating no statistically significant differences in average heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730cb6e8-b30b-4e5c-a7e4-4d60535feb63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
